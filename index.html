<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Spot is powered by large language models to chain low-level skills to achieve language rearrangement tasks.">
  <meta name="keywords" content="Large Language Models, Open-vocabulary pick-and-place">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Open-vocabulary Mobile Pick-and-Place</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src=""></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Open-vocabulary Mobile Pick-and-Place</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sites.google.com/view/tyjimmyyang">Tsung-Yen Yang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/SergioArnaud">Sergio Arnaud</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=p463opcAAAAJ&hl=en">Alexander William Clegg</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.ericundersander.com/">Eric Undersander</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://roozbehm.info/">Roozbeh Mottaghi</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=DMTuJzAAAAAJ&hl=en">Mrinal Kalakrishnan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://faculty.cc.gatech.edu/~dbatra/">Dhruv Batra</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://akshararai.github.io/">Akshara Rai</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Meta AI, FAIR Team</span>
            <span class="author-block"><sup>2</sup>Georgia Tech</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <br>
        <div class="videoWrapper">
          <iframe width="2560" height="1135"
            src="https://www.youtube.com/embed/cQIQKCt0YIY" title="Open-vocabulary Mobile Pick-and-Place for Spot" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
          </iframe>
        </div>
        <div class="content has-text-justified">
          <p>
            Natural language provides a simple way for humans to specify a goal for a robot,
            such as <i>"go to the kitchen and bring the sugar to me."</i>
            However, current state-of-the-art language-based embodied AI systems
            such as SayCan do not have such capability since
            (1) they rely on a fixed-set vocabulary that cannot generalize to diverse instructions,
            and (2) the robot does not know the semantic meaning of the environment,
            and thus requires knowing point goal location to navigate to.
          </p>
          <p>
            In this post, we demonstrate embodied AI systems that use large language models (LLMs)
            to chain Spot's low-level skills and thus complete open-vocabulary object nav-pick-place tasks.
            The proposed system consists of three parts:
            (1) LLMs take a natural language instruction as input and produce a sequence of low-level skills,
            along with their corresponding skills' input parameters such as the target object text,
            (2) a pre-built map that stores CLIP embedding of the environment is used to query the point goal location given text generated from LLMs,
            and (3) finally these low-level skills are executed sequentially to complete the task.
          </p>
          <p>
            By doing so, the robot figures out the location of the target object and formulates a plan of action.
            These components form a natural hierarchical architecture for intelligent control:
            a high level at which goal and state are identified from sensory data
            and object search and feasible plans are formulated given text,
            and a low level at which skills are chained together.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Demo</h2>
        <div class="content has-text-justified">
          <p>
            In this demo, a user types <i>"find human and get mug and place it in the case"</i>,
            LLMs take the text input and outputs a sequence of low-level skills:
            <b>NavPolicy(<i>"human"</i>) -> PickPolicy(<i>"mug"</i>) -> NavPolicy(<i>"case"</i>) -> PlacePolicy()</b>.
            When executing <b>NavPolicy(<i>"human"</i>)</b>,
            a semantic feature map of the environment is queried to get a point-goal location of the human.
            Then this point goal location is fed into a reinforcement-learning-trained navigation policy
            to let Spot navigate to the target. After Spot finds the human,
            <b>PickPolicy(<i>"mug"</i>)</b> is used to pick up the target object.
            The pick policy is trained to move the arm to the target grasping location of the object
            defined by the text so that Spot can grasp the object.
            Finally, Spot uses <b>NavPolicy(<i>"case"</i>)</b> to find the case and then calls
            <b>PlacePolicy()</b> to place the object there.
            In addition, Spot is able to take an alternative route to avoid collision with humans as shown in the micro-kitchen demo.
            This shows that Spot is robust to disturbance.
          </p>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Future Direction</h2>
        <div class="content has-text-justified">
          <p>
            The current demo still does not consider the case where the object is not static
            (locations changed over time), LLMs need to replan due to disturbance,
            and when a pre-built map of the environment is not provided.
            We want to keep scaling our research and infrastructure to multiple scenes and explore
            other Spot's low-level skills such as opening the door and pouring the coffee into a mug.
            In addition, we are continuously exploring LLMs for driving decision-making
            for robots such as grounding LLMs in the state of the robot to account for disturbance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Acknowledgment</h2>
        <div class="content has-text-justified">
          <p>
            This demo pieces many efforts together within the Embodied AI team at FAIR.
            One critical component of this demo is low-level skills. These low-level skills are from
            <a href="https://adaptiveskillcoordination.github.io/">Adaptive Skill Coordination for Robotic Mobile Manipulation paper</a>.
            In addition, this demo uses LLMs to plan a sequence of low-level skills.
            Sergio Arnaud and Oleksandr Maksymets lead the project of using LLMs in
            <a href="https://aihabitat.org/">Habitat</a>.
            Finally, we also use the idea from the paper
            <a href="https://arxiv.org/abs/2212.00922">Navigating to Objects in the Real World</a>
            to get the map of the environment. We thank Christopher Paxton for the helpful discussion.
            Finally, We thank all the people in the Embodied AI team who provided important feedback for this demo.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
